{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfba9429-0093-479f-b454-afaea7de620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load err analysis data\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class ErrorAnalysisDataset(Dataset):\n",
    "    def __init__(self, dataset_root, pred_split, img_root_dir=None, transform=None):\n",
    "\n",
    "        self.data = pd.read_csv(os.path.join(dataset_root, \"dataset.csv\"))\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(os.path.join(dataset_root, 'mapping.json'), 'r') as f:\n",
    "            self.mapping = json.load(f)\n",
    "            idx_to_mapping = list(self.mapping)\n",
    "            \n",
    "        predictions = pd.read_csv(os.path.join(dataset_root, f\"pred_splits/{pred_split}\"), header=None, names=['pred'])\n",
    "        self.data['pred']=predictions['pred'].values\n",
    "        self.data['pred']=self.data['pred'].apply(lambda pred: self.mapping[idx_to_mapping[pred]])\n",
    "        self.data['gt']=self.data['gt'].apply(lambda pred: self.mapping[idx_to_mapping[pred]])\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def load_image(self, img_path):\n",
    "        img_path = os.path.join(self.img_root_dir, img_path)\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_root_dir, self.data.iloc[idx]['img_id'])\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Extract metadata\n",
    "        attribute = self.data.iloc[idx]['attribute']\n",
    "        gt_code = self.data.iloc[idx]['gt_code']\n",
    "        gt = self.data.iloc[idx]['gt']\n",
    "        pred = self.data.iloc[idx]['pred']\n",
    "\n",
    "        # Return a dictionary with the image and metadata\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            # 'attribute': attribute,\n",
    "            # 'gt_code': gt_code,\n",
    "            # 'gt': gt,\n",
    "            # 'pred': pred\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26750db1-d251-4372-bd25-0456d6ab0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Context class that you can add to and read from, should be LLM ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d5b2e2-14b5-4cf3-9726-fc9cce189b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix analysis module\n",
    "import numpy as np\n",
    "class InitialAnalysis:\n",
    "    def __init__(self, df, prediction_col, ground_truth_col, k=5):\n",
    "        # Get unique classes\n",
    "        classes = sorted(df[ground_truth_col].unique())\n",
    "        \n",
    "        # Initialize confusion matrix\n",
    "        confusion_matrix = pd.DataFrame(\n",
    "            np.zeros((len(classes), len(classes)), dtype=int),\n",
    "            index=classes,\n",
    "            columns=classes\n",
    "        )\n",
    "        \n",
    "        # Populate the confusion matrix\n",
    "        for _, row in df.iterrows():\n",
    "            actual = row[ground_truth_col]\n",
    "            predicted = row[prediction_col]\n",
    "            confusion_matrix.loc[actual, predicted] += 1\n",
    "        \n",
    "        # Extract non-diagonal elements\n",
    "        errors = []\n",
    "        for actual in classes:\n",
    "            for predicted in classes:\n",
    "                if actual != predicted and confusion_matrix.loc[actual, predicted] > 0:\n",
    "                    errors.append(((actual, predicted), confusion_matrix.loc[actual, predicted]))\n",
    "\n",
    "        # Sort errors by count and take top k\n",
    "        top_k_errors = sorted(errors, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.top_k_errors_pred_conditional = top_k_errors\n",
    "        self.k = k\n",
    "        self.classes = classes\n",
    "        \n",
    "    def human_readable_topk_pred_conditional_errors(self):\n",
    "        errors_nl = []\n",
    "        for (actual, predicted), err_count in self.top_k_errors_pred_conditional:\n",
    "            errors_nl.append(f\"The actual class is '{actual}', however model incorrectly predicts '{predicted}' {err_count} times\")\n",
    "            \n",
    "        return '\\n '.join(errors_nl)\n",
    "    \n",
    "    def human_readable_topk_errors_gt(self):\n",
    "        marginal_errs = []\n",
    "        for gt in self.classes:\n",
    "            marginal_errs.append((gt, (self.confusion_matrix.loc[gt].sum()-self.confusion_matrix.loc[gt, gt]).item()))\n",
    "\n",
    "        # select top k\n",
    "        marginal_errs = sorted(marginal_errs, key=lambda x: x[1], reverse=True)[:self.k]\n",
    "        \n",
    "        return f\"The top five marginal errors are for these classes: {marginal_errs}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dca2b4a-3fff-4459-bad2-2c005ff47279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>img_id</th>\n",
       "      <th>attribute</th>\n",
       "      <th>gt_code</th>\n",
       "      <th>gt</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n01443537/art_0.jpg</td>\n",
       "      <td>art</td>\n",
       "      <td>n01443537</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>missile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>n01443537/art_1.jpg</td>\n",
       "      <td>art</td>\n",
       "      <td>n01443537</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>n01443537/art_10.jpg</td>\n",
       "      <td>art</td>\n",
       "      <td>n01443537</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>soccer_ball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>n01443537/art_11.jpg</td>\n",
       "      <td>art</td>\n",
       "      <td>n01443537</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>n01443537/art_12.jpg</td>\n",
       "      <td>art</td>\n",
       "      <td>n01443537</td>\n",
       "      <td>goldfish</td>\n",
       "      <td>goldfish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                img_id attribute    gt_code        gt  \\\n",
       "0           0   n01443537/art_0.jpg       art  n01443537  goldfish   \n",
       "1           1   n01443537/art_1.jpg       art  n01443537  goldfish   \n",
       "2           2  n01443537/art_10.jpg       art  n01443537  goldfish   \n",
       "3           3  n01443537/art_11.jpg       art  n01443537  goldfish   \n",
       "4           4  n01443537/art_12.jpg       art  n01443537  goldfish   \n",
       "\n",
       "          pred  \n",
       "0      missile  \n",
       "1     goldfish  \n",
       "2  soccer_ball  \n",
       "3     goldfish  \n",
       "4     goldfish  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it out\n",
    "dataset = ErrorAnalysisDataset(dataset_root='../mock_data_creation/mock_data', pred_split='split_0.txt', img_root_dir='/ix/akovashka/arr159/imagenet-r')\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8a0762-4aa8-41d2-8dc4-fa1cf832ba21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The top five marginal errors are for these classes: [('mushroom', 125), ('toucan', 97), ('flamingo', 96), ('bee', 81), ('jellyfish', 80)]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = InitialAnalysis(dataset.data, prediction_col='pred', ground_truth_col='gt')\n",
    "analysis.human_readable_topk_errors_gt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a4d5a5-956c-4966-a9c1-743ec24330a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The actual class is 'ant', however model incorrectly predicts 'burrito' 4 times\\n The actual class is 'hotdog', however model incorrectly predicts 'west_highland_white_terrier' 4 times\\n The actual class is 'mushroom', however model incorrectly predicts 'pig' 4 times\\n The actual class is 'tank', however model incorrectly predicts 'parachute' 4 times\\n The actual class is 'African_chameleon', however model incorrectly predicts 'shih_tzu' 3 times\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.human_readable_topk_pred_conditional_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b39137-87d0-4924-a646-fe23b19e0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, slice_condition, prediction_col='pred', ground_truth_col='gt', n=10):\n",
    "    # filter data based on slice condition\n",
    "    filtered_data = data[data.apply(slice_condition, axis=1)]\n",
    "    \n",
    "    # divide into error set and non error set\n",
    "    error_set = filtered_data[filtered_data[prediction_col] != filtered_data[ground_truth_col]]\n",
    "    non_error_set = filtered_data[filtered_data[prediction_col] == filtered_data[ground_truth_col]]\n",
    "\n",
    "    # sample n from both sets\n",
    "    sampled_error = error_set.sample(n=min(len(error_set), n), random_state=42)  # Use random_state for reproducibility\n",
    "    sampled_non_error = non_error_set.sample(n=min(len(non_error_set), n), random_state=42)\n",
    "\n",
    "    return sampled_error['img_id'].values, sampled_non_error['img_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec6cd87-acef-446d-947f-3c5e3932bfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['n04086273/art_3.jpg', 'n02108915/misc_42.jpg',\n",
       "        'n01882714/toy_15.jpg', 'n07697537/misc_118.jpg',\n",
       "        'n02356798/cartoon_11.jpg', 'n02094433/misc_37.jpg',\n",
       "        'n07873807/misc_6.jpg', 'n02480855/tattoo_28.jpg',\n",
       "        'n02007558/embroidery_5.jpg', 'n10565667/sketch_3.jpg'],\n",
       "       dtype=object),\n",
       " array(['n03649909/misc_0.jpg', 'n01498041/misc_1.jpg',\n",
       "        'n07768694/toy_0.jpg', 'n02134084/misc_119.jpg',\n",
       "        'n01518878/origami_7.jpg', 'n01833805/misc_4.jpg',\n",
       "        'n02802426/tattoo_6.jpg', 'n02769748/cartoon_4.jpg',\n",
       "        'n01632777/sketch_17.jpg', 'n01843383/embroidery_1.jpg'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(dataset.data, lambda x: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d9c336-06f2-40e7-9742-ce19a7ac4b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2281ac18fc204be6aa55af112630fb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load mllm model\n",
    "MAX_LENGTH = 384\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        # _attn_implementation=\"flash_attention_2\",\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1bff6c-6778-48b0-8c7c-258f32ca862c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m images \u001b[38;5;241m=\u001b[39m [dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredictions\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "images = [dataset[0]['image']] # testing\n",
    "texts = []\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de82ee-30d7-4b85-a4e4-0c85c40d89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captioning, prompt for caption Caption the image with the most salient details\n",
    "\n",
    "def caption_sets(model, processor, dataset, err_list, non_error_list, prompt=\"Caption the image with the most salient details\"):\n",
    "    images = []\n",
    "    texts=[]\n",
    "    images.extend([dataset.load_image(img_path) for img_path in err_list])\n",
    "    images.extend([dataset.load_image(img_path) for img_path in non_error_list])\n",
    "\n",
    "    for img in images:\n",
    "        prompt = f\"USER: <image>\\nCaption the image with the most salient details.\\nASSISTANT:\"\n",
    "        texts.append(prompt)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"].cuda()\n",
    "    attention_mask = batch[\"attention_mask\"].cuda()\n",
    "    pixel_values = batch[\"pixel_values\"].cuda()\n",
    "    \n",
    "    generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                           pixel_values=pixel_values, max_new_tokens=MAX_LENGTH)\n",
    "    \n",
    "    predictions = processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "    \n",
    "    err_list_prompt = [f'A sample with a incorrect prediction has description: ' for i, _ in enumerate(err_list)]\n",
    "    non_err_list_prompt = [f'A sample with a correct prediction has description: ' for i, _ in enumerate(non_error_list)]\n",
    "    return [f'{text}{predictions[i]}' for i, text in enumerate([*err_list_prompt, *non_err_list_prompt])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8e337-026e-4712-9f1a-8f0a040ab67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "err_list, non_err_list = sample(dataset.data, lambda x: True)\n",
    "\n",
    "caption_sets(model, processor, dataset, err_list, non_err_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7616fba9-2199-46c9-afa1-cc12c3368cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1207 14:09:23.280295343 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/akovashka/arr159/miniconda3/envs/sports_feedback2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115564/work/torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 4.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# load LLM\n",
    "import os\n",
    "\n",
    "# Set the master address and port\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "from llama_models.llama3.reference_impl.generation import Llama\n",
    "from llama_models.llama3.api.datatypes import (\n",
    "    CompletionMessage,\n",
    "    StopReason,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "\n",
    "temperature = 0.6\n",
    "top_p = 0.9\n",
    "max_seq_len = 10000\n",
    "max_batch_size = 4\n",
    "max_gen_len = None\n",
    "model_parallel_size = None\n",
    "tokenizer_path = str(\"/ihome/akovashka/arr159/sports_feedback/llama-models/models/llama3/api/tokenizer.model\")\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=\"/ihome/akovashka/arr159/.llama/checkpoints/Llama3.1-8B-Instruct\",\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    "    model_parallel_size=model_parallel_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a88cdd4-edfb-4d4c-88a5-70a2d2edc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis + validator class\n",
    "\n",
    "\"\"\"\n",
    "Hypothesis Formulation:\n",
    "Uses a LLM to formulate (1) a hypothesis on what might be a possible error-prone \n",
    "attribute given the context and (2) a labeling prompt given the generated hypothesis \n",
    "(e.g. given hypothesis \"The model is likely misclassifying images in poorly lit \n",
    "conditions because details of the object are obscured.\", the labeling prompt would be \n",
    "\"For each image, label whether the lighting condition is: [1] 'Bright', [2] 'Dim', \n",
    "or [3] 'Dark'.\")\n",
    "\n",
    "Validation:\n",
    "Using an M-LLM, the images in each set are labeled using the labeling prompt from the \n",
    "hypothesis formulation step (constrains the labeling space), and then error-rates are \n",
    "re-computed for each attribute in the labeled subset (e.g. given the hypothesis/prompt \n",
    "example above, the results could be Bright: 0.75 ER, Dim: 0.2 ER, Dark: 0.25 ER; note \n",
    "this doesnâ€™t need to sum to 1) and this result is passed to the context.\n",
    "\"\"\"\n",
    "\n",
    "class ExperimentFlow:\n",
    "    def __init__(self, llm, mllm=None, verbose=False):\n",
    "        self.llm = llm\n",
    "        self.mllm = mllm\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def form_hypothesis(self, context, \n",
    "                        prompt='''Formulate a hypothesis on why the misclassified images might be failing.'''):\n",
    "        \n",
    "        dialog = [\n",
    "                SystemMessage(content=prompt),\n",
    "                UserMessage(content=f'''Context: \"The actual class is 'ant', however model incorrectly predicts 'burrito' 4 times\\n The actual class is 'hotdog', however model incorrectly predicts 'west_highland_white_terrier' 4 times\\n The actual class is 'mushroom', however model incorrectly predicts 'pig' 4 times\\n The actual class is 'tank', however model incorrectly predicts 'parachute' 4 times\\n The actual class is 'African_chameleon', however model incorrectly predicts 'shih_tzu' 3 times\" Description: \"A sample with an incorrect prediction has description: The image features a woman wearing a yellow shirt with a gun design on it. The shirt has a green and red color scheme, and the gun is prominently displayed on the front. The image is taken in a dimly lit room, making the colors appear muted and details of the gun design less distinct.\"\n",
    "                    \"A sample with an incorrect prediction has description: The image is a black and white drawing of a small dog with a bow tie. The drawing was photographed under low light, causing shadows to obscure some details of the bow tie.\"\n",
    "                    \"A sample with a correct prediction has description: The image features a cartoon-like drawing of a lizard with a sad expression. Despite being in low light, the high contrast in the drawing ensures that the lizard's details remain visible.\"\n",
    "                    \"A sample with a correct prediction has description: The image features a colorful, hand-woven blanket with a bird design. The bright and vibrant colors make the details of the bird clear, even under dim lighting.\" \\nHypothesis:'''),\n",
    "                CompletionMessage(\n",
    "                    content=\"\"\"The model is likely misclassifying images in poorly lit conditions because details of the object are obscured.\"\"\",\n",
    "                    stop_reason=StopReason.end_of_turn,\n",
    "                ),\n",
    "                UserMessage(content=f'\\nContext: {context}\\nHypothesis: '),\n",
    "            ]\n",
    "        result = self.llm.chat_completion(\n",
    "            dialog,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        if self.verbose:\n",
    "            for msg in dialog:\n",
    "                print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n",
    "    \n",
    "        return result.generation.content\n",
    "\n",
    "    def get_labeling_prompt(self, hypothesis, prompt=\"Create a extremely concise labeling prompt given a hypothesis\"):\n",
    "        dialog = [\n",
    "                SystemMessage(content=prompt),\n",
    "                UserMessage(content=f'Hypothesis: The model is likely misclassifying images in poorly lit conditions because details of the object are obscured.\\nLabeling Prompt: '),\n",
    "                CompletionMessage(\n",
    "                    content=\"\"\"For each image, label from two options whether the lighting condition is: [1] 'Bright', [2] 'Dim/Dark'.\"\"\",\n",
    "                    stop_reason=StopReason.end_of_turn,\n",
    "                ),\n",
    "                UserMessage(content=f'Hypothesis: {hypothesis}\\nLabeling Prompt: ')\n",
    "        ]\n",
    "\n",
    "        result = self.llm.chat_completion(\n",
    "            dialog,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            for msg in dialog:\n",
    "                print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n",
    "    \n",
    "        return result.generation.content\n",
    "    def label_images(self, image_list, labeling_prompt):\n",
    "        images = [dataset.load_image(img_path) for img_path in image_list]\n",
    "        # texts=[]\n",
    "        predictions = []\n",
    "        # self.mllm['model'].eval()\n",
    "\n",
    "        # batch size 1 due to GPU mem limit with LLM\n",
    "        for img in images:\n",
    "            prompt = f\"USER: <image>\\n {labeling_prompt}\\nASSISTANT:\"\n",
    "            # texts.append(prompt)\n",
    "    \n",
    "            batch = self.mllm['processor'](text=[prompt], images=[img], return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].cuda()\n",
    "            attention_mask = batch[\"attention_mask\"].cuda()\n",
    "            pixel_values = batch[\"pixel_values\"].cuda()\n",
    "        \n",
    "            generated_ids = self.mllm['model'].generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                                   pixel_values=pixel_values, max_new_tokens=100)\n",
    "            \n",
    "            pred = self.mllm['processor'].batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "            predictions.append(pred[0])\n",
    "        return predictions\n",
    "    \n",
    "    def validate(self, labeling_prompt, error_img_list, correct_img_list):\n",
    "        def parse_categories(labeling_prompt):\n",
    "            # Extract categories from the prompt\n",
    "            categories = []\n",
    "            lines = labeling_prompt.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if \"[\" in line and \"]\" in line:\n",
    "                    # Extract text between [number] and the label\n",
    "                    category = line.split(\"]\")[1].strip(\" '\")\n",
    "                    categories.append(category)\n",
    "            return categories\n",
    "            \n",
    "        def get_match():\n",
    "            for category in categories:\n",
    "                if category.lower() in prediction.lower():\n",
    "                    return category\n",
    "            return \"Unknown\"\n",
    "            \n",
    "        def calc_err_rate(attribute_classifications, err_list, categories):\n",
    "            # 1 represents error in err_list\n",
    "            category_errors = {category: 0 for category in categories}\n",
    "            total_count = {category: 0 for category in categories}\n",
    "        \n",
    "            # Calculate errors and counts for each category\n",
    "            for i, classification in enumerate(attribute_classifications):\n",
    "                if classification in categories:\n",
    "                    total_count[classification] += 1\n",
    "                    if i < len(err_list):  # Error list indices\n",
    "                        category_errors[classification] += 1\n",
    "            \n",
    "            # Calculate error rate per category\n",
    "            error_rates = {\n",
    "                category: (category_errors[category] / total_count[category]) if total_count[category] > 0 else 0\n",
    "                for category in categories\n",
    "            }\n",
    "            return error_rates\n",
    "            \n",
    "        categories = parse_categories(labeling_prompt)\n",
    "        predictions = self.label_images([*error_img_list, *correct_img_list], labeling_prompt)\n",
    "        \n",
    "        for i, _ in enumerate(error_img_list):\n",
    "            attribute_classifications.append(get_match(predictions[i], categories))\n",
    "        for i, _ in enumerate(correct_img_list):\n",
    "            attribute_classifications.append(get_match(predictions[i+len(error_img_list)], categories))\n",
    "        \n",
    "        error_rates = calc_err_rate(attribute_classifications, [1 if i < len(error_img_list) else 0 for i in range(len(error_img_list)+len(correct_img_list))], categories)\n",
    "\n",
    "        # validate hypothesis based on err_rates calculated\n",
    "        return error_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49ce4832-7f70-4c31-90b2-58626d82f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "experiment = ExperimentFlow(generator, mllm={'processor': processor, 'model':model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f178b609-51cc-4aa1-bbc9-9e142c90f476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract',\n",
       " '[1] Cartoon/Abstract']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labeling_prompt='''For each image, label one of the following categories:\n",
    "[1] **Cartoon/Abstract**: Image is cartoonish, abstract, or lacks detail. \n",
    "[2] **Realistic/High-Res**: Image is realistic, high-resolution, and detailed.\n",
    "[3] **Other**: Image does not fit into the above categories.'''\n",
    "\n",
    "test_labeling_prompt='''For each image, label one of the following categories:\n",
    "[1] Cartoon/Abstract [2] Realistic/High-Res [3] Other'''\n",
    "\n",
    "experiment.label_images(sample(dataset.data, lambda x: True)[0], test_labeling_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd1a64e9-a131-4751-82c8-a1b539c91efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output hypothesis: The model is struggling to classify images that are:\n",
      "\n",
      "1. Cartoony or stylized (e.g. dog with exaggerated ears, abstract apple painting)\n",
      "2. Abstract or lacking in detail (e.g. apple with no stem or leaf, abstract painting)\n",
      "3. Lacking in texture or realism (e.g. dog's fur not visible)\n",
      "\n",
      "In contrast, the model is performing well on images that are:\n",
      "\n",
      "1. Realistic and high-resolution (e.g. car photograph, tree photograph)\n",
      "2. Detailed and textured (e.g. tree bark, leaves, and branches)\n",
      "\n",
      "This suggests that the model may be relying heavily on texture and detail to make accurate predictions, and may struggle with images that lack these characteristics.\n",
      "Output labeling prompt: For each image, label one of the following categories:\n",
      "\n",
      "[1] **Cartoon/Abstract**: Image is cartoonish, abstract, or lacks detail.\n",
      "[2] **Realistic/High-Res**: Image is realistic, high-resolution, and detailed.\n",
      "[3] **Other**: Image does not fit into the above categories.\n",
      "\n",
      "Optional: Additionally, label the presence or absence of texture: [T] **Textured**, [F] **Not Textured**.\n"
     ]
    }
   ],
   "source": [
    "# hypothesis test -> for artistic rendition\n",
    "test_context = '''\n",
    "\"The top five marginal errors are for these classes: [('dog', 125), ('apple', 97), ('flamingo', 96), ('bee', 81), ('jellyfish', 80)]\"\n",
    "Description:\n",
    "\"A sample with an incorrect prediction has description: The image is a cartoon-style drawing of a dog with long, exaggerated ears and a pointed snout. The dog is depicted with flat colors and no fur details, resembling a fox's silhouette.\n",
    "A sample with an incorrect prediction has description: The image is an abstract painting of an apple, with its round shape emphasized and no clear stem or leaf visible, making it appear similar to a ball.\n",
    "A sample with a correct prediction has description: The image is a realistic photograph of a car taken in bright lighting, with clear details like the brand logo, tires, and windows visible.\n",
    "A sample with a correct prediction has description: The image is a high-resolution photograph of a tree in a park, with sharp details in the bark, leaves, and branches, making it distinct from abstract or cartoon-like styles.\"\n",
    "'''\n",
    "test_hypothesis = experiment.form_hypothesis(test_context)\n",
    "print(\"Output hypothesis:\", test_hypothesis)\n",
    "test_labeling_prompt = experiment.get_labeling_prompt(test_hypothesis)\n",
    "print(\"Output labeling prompt:\", test_labeling_prompt)\n",
    "\n",
    "# so far so good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e415c2-1e17-473d-b3b1-8c94a22fb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion; maybe just use LLM-brain, find prompt? max steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7724c04-40dc-46fc-9764-df910af05880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm brain class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
